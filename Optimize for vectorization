import pathlib
import json
import re
import logging
import uuid
from datetime import datetime
from typing import List, Dict, Any

# --- 1. CONFIGURATION ---

class ChunkingConfig:
    # Paths
    BASE_DIR = pathlib.Path(r"C:\College Notes DATA_BASE")
    INPUT_DIR = BASE_DIR / "Transcribed_Text"
    OUTPUT_FILE = BASE_DIR / "knowledge_base.json"
    
    # Tuning Parameters
    MIN_CHUNK_LENGTH = 50   # Discard chunks with fewer than 50 characters (noise)
    MAX_CHUNK_LENGTH = 1500 # Soft limit for chunk size (approx 300-400 tokens)

logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - [%(levelname)s] - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("SemanticChunker")

# --- 2. THE CHUNKING ENGINE ---

class NoteParser:
    """
    A state-machine parser that reads text line-by-line to build 
    semantically coherent chunks based on Markdown headers.
    """
    def __init__(self):
        # Regex to detect headers (e.g., "# Topic" or "## Subtopic")
        self.header_pattern = re.compile(r'^(#{1,3})\s+(.+)$')
        # Regex to detect page markers (e.g., "## Page 5")
        self.page_pattern = re.compile(r'^##\s+Page\s+(\d+)$', re.IGNORECASE)
        # Regex for cleaning multiple spaces
        self.whitespace_pattern = re.compile(r'\s+')

    def parse_file(self, file_path: pathlib.Path) -> List[Dict[str, Any]]:
        """
        Parses a single text file into a list of chunk objects.
        """
        chunks = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except Exception as e:
            logger.error(f"Could not read {file_path.name}: {e}")
            return []

        # State Variables
        current_chunk_text = []
        current_header = "Introduction" # Default header if none exists at top
        current_page = 1
        
        source_pdf = file_path.stem + ".pdf"

        for line in lines:
            stripped_line = line.strip()
            if not stripped_line:
                continue

            # Check 1: Is this a Page Marker?
            page_match = self.page_pattern.match(stripped_line)
            if page_match:
                # Update state, but do NOT add to text (keeps flow continuous)
                current_page = int(page_match.group(1))
                continue

            # Check 2: Is this a Topic Header?
            header_match = self.header_pattern.match(stripped_line)
            if header_match:
                # If we have accumulated text, save the PREVIOUS chunk
                if current_chunk_text:
                    self._finalize_chunk(chunks, current_chunk_text, current_header, source_pdf, current_page)
                    current_chunk_text = []
                
                # Update the header for the NEXT chunk
                # We strip extra characters from the header title itself
                current_header = header_match.group(2).strip()
                continue

            # Default: It's content. Append to buffer.
            current_chunk_text.append(stripped_line)

        # Catch the final chunk after the loop ends
        if current_chunk_text:
            self._finalize_chunk(chunks, current_chunk_text, current_header, source_pdf, current_page)

        return chunks

    def _clean_text(self, text_lines: List[str]) -> str:
        """
        Sanitizes text for better embedding quality.
        """
        # 1. Join lines with space to fix sentence breaks
        full_text = " ".join(text_lines)
        
        # 2. Collapse multiple spaces/tabs into single space
        full_text = self.whitespace_pattern.sub(' ', full_text)
        
        # 3. Remove common Markdown artifacts that don't help meaning
        # (e.g., "**" bold markers, "==" highlights)
        # We keep the text inside, just remove the symbols
        full_text = full_text.replace("**", "").replace("__", "")
        
        # 4. Normalize quotes (optional, but good for consistency)
        full_text = full_text.replace('"', '"').replace('"', '"')
        
        return full_text.strip()

    def _finalize_chunk(self, chunk_list, text_lines, header, source, page):
        """
        Helper to construct the chunk dictionary and append it if valid.
        """
        # Run the cleaner
        clean_content = self._clean_text(text_lines)
        
        # Filter 1: Noise Reduction (Too short)
        if len(clean_content) < ChunkingConfig.MIN_CHUNK_LENGTH:
            return

        # Filter 2: Split Oversized Chunks
        # If a chunk is massive, we can split it. 
        # For now, we log it. In production, you might want to split strictly by tokens.
        if len(clean_content) > ChunkingConfig.MAX_CHUNK_LENGTH:
            logger.warning(f"Oversized chunk detected in {source}: {header} ({len(clean_content)} chars)")

        chunk_obj = {
            "id": str(uuid.uuid4()),
            "source": source,
            "page_start": page, # Renamed to page_start to be clearer
            "topic": header,
            "content": clean_content,
            "length": len(clean_content),
            "timestamp": datetime.now().isoformat()
        }
        chunk_list.append(chunk_obj)

# --- 3. MAIN EXECUTION ---

def build_knowledge_base():
    if not ChunkingConfig.INPUT_DIR.exists():
        logger.error(f"Input directory not found: {ChunkingConfig.INPUT_DIR}")
        return

    parser = NoteParser()
    master_knowledge_base = []
    
    # Gather all text files
    txt_files = list(ChunkingConfig.INPUT_DIR.glob("*.txt"))
    logger.info(f"Found {len(txt_files)} documents to segregate.")

    for txt_file in txt_files:
        try:
            file_chunks = parser.parse_file(txt_file)
            master_knowledge_base.extend(file_chunks)
            logger.info(f"Processed {txt_file.name}: {len(file_chunks)} chunks extracted.")
        except Exception as e:
            logger.error(f"Failed to parse {txt_file.name}: {e}")

    # Output Statistics
    if not master_knowledge_base:
        logger.warning("No valid chunks were generated.")
        return

    logger.info(f"--- SEGMENTATION COMPLETE ---")
    logger.info(f"Total Chunks Generated: {len(master_knowledge_base)}")
    
    # Save to JSON
    try:
        with open(ChunkingConfig.OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(master_knowledge_base, f, indent=2, ensure_ascii=False)
        logger.info(f"Knowledge Base saved to: {ChunkingConfig.OUTPUT_FILE}")
    except Exception as e:
        logger.error(f"Failed to write JSON output: {e}")

if __name__ == "__main__":
    build_knowledge_base()
