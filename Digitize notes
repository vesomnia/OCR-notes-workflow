import pathlib
import time
import os
import csv
import logging
import re
from collections import deque  # Added for the buffer
from datetime import datetime
from google import genai
from google.api_core import exceptions
from google.genai import types

# --- 1. CONFIGURATION & CONSTANTS ---

# Centralized configuration for easier maintenance
class IngestionConfig:
    API_KEY = os.getenv("GEMINI_API_KEY")
    # Use raw strings for Windows paths to avoid escape character conflicts
    BASE_DIR = pathlib.Path(r"C:\College Notes DATA_BASE")
    OUTPUT_DIR = BASE_DIR / "Transcribed_Text"
    QUARANTINE_DIR = OUTPUT_DIR / "_Review_Required"
    LOG_PATH = BASE_DIR / "conversion_failures.csv"
    
    # Model definitions
    MODEL_FAST = "gemini-2.5-flash" 
    
    # Generation parameters
    TEMP_TRANSCRIPTION = 0.1
    TEMP_AUDIT = 0.0

    # Rate Limiting (Free Tier Buffer)
    RPM_LIMIT = 5   # Max requests per minute
    RPM_PERIOD = 60 # Time window in seconds

# Initialize Logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - [%(levelname)s] - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("AcademicArchivist")

if not IngestionConfig.API_KEY:
    raise EnvironmentError("Critical: GEMINI_API_KEY environment variable is missing.")

client = genai.Client(api_key=IngestionConfig.API_KEY)

# --- 2. RATE LIMITER (THE BUFFER) ---

class RateLimiter:
    """
    A sliding window rate limiter to prevent API 429 errors.
    It tracks the timestamps of the last N requests and sleeps
    if the limit is about to be exceeded.
    """
    def __init__(self, max_calls, period=60):
        self.max_calls = max_calls
        self.period = period
        self.timestamps = deque()

    def wait_for_slot(self):
        """
        Blocks execution until a slot is available in the sliding window.
        """
        now = time.time()
        
        # Remove timestamps older than the period window
        while self.timestamps and now - self.timestamps[0] > self.period:
            self.timestamps.popleft()
            
        # If we have reached the limit, calculate sleep time
        if len(self.timestamps) >= self.max_calls:
            # How long until the oldest request expires?
            sleep_time = self.period - (now - self.timestamps[0])
            if sleep_time > 0:
                logger.info(f"Rate Limit Buffer: Pausing for {sleep_time:.2f}s...")
                time.sleep(sleep_time + 0.1) # Add tiny buffer to be safe
            
            # Clean up the deque again after sleeping
            self.timestamps.popleft()
            
        # Record the current request
        self.timestamps.append(time.time())

# Initialize the global limiter
api_limiter = RateLimiter(max_calls=IngestionConfig.RPM_LIMIT, period=IngestionConfig.RPM_PERIOD)

# --- 3. LOGGING & AUDIT TOOLS ---

def log_failure(filename, stage, error_msg):
    """
    Records operational failures to a persistent CSV ledger.
    """
    file_exists = IngestionConfig.LOG_PATH.exists()
    try:
        with open(IngestionConfig.LOG_PATH, "a", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=["timestamp", "file", "stage", "error"])
            if not file_exists: 
                writer.writeheader()
            writer.writerow({
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "file": filename, 
                "stage": stage, 
                "error": str(error_msg)
            })
    except IOError as e:
        logger.error(f"Failed to write to failure log: {e}")

def validate_syntax(text_content):
    """
    Performs a heuristic audit of LaTeX delimiters.
    
    It filters out escaped dollars (e.g., \$) used for currency 
    before calculating the parity of mathematical delimiters.
    """
    # Remove escaped dollars to prevent false positives (e.g., "\$50")
    clean_text = text_content.replace(r"\$", "")
    
    # Count remaining dollars. In standard LaTeX, these should effectively appear in pairs.
    delimiter_count = clean_text.count("$")
    
    if delimiter_count % 2 != 0:
        return False, f"Detected odd number of LaTeX delimiters ({delimiter_count})."
    
    return True, "Valid"

# --- 4. CORE PROCESSING LOGIC ---

def process_document(pdf_path, output_path):
    """
    Orchestrates the conversion of a PDF into a verified Markdown artifact.
    
    Phase 1: Ingestion & Transcription (Vision-to-Text)
    Phase 2: Technical Audit (Text-to-Text Verification)
    """
    myfile = None
    try:
        # Step A: Server-Side Ingestion
        myfile = client.files.upload(file=pdf_path)
        
        # Wait for file readiness
        start_time = time.time()
        while myfile.state == "PROCESSING":
            if time.time() - start_time > 300:
                raise TimeoutError("Server ingestion timed out.")
            time.sleep(2)
            myfile = client.files.get(name=myfile.name)

        if myfile.state == "FAILED":
            raise ValueError("File failed internal server validation.")

        # Step B: Primary Transcription
        transcription_prompt = (
            "SYSTEM: You are an Academic Archivist specializing in paleography. "
            "Your task is to transcribe these handwritten notes into clean Markdown. "
            "Rules:\n"
            "1. Transcribe text exactly as written. If a word is illegible, mark it as [?].\n"
            "2. Convert all mathematical expressions into standard LaTeX notation (e.g., $E=mc^2$).\n"
            "3. Do not summarize or explain the content. Literal transcription only.\n"
            "4. Preserve the hierarchy of headers and bullet points."
        )

        config_transcribe = types.GenerateContentConfig(
            temperature=IngestionConfig.TEMP_TRANSCRIPTION,
            max_output_tokens=8192
        )

        # --- BUFFER CHECK INSERTED HERE ---
        api_limiter.wait_for_slot()
        
        draft_response = client.models.generate_content(
            model=IngestionConfig.MODEL_FAST,
            contents=[myfile, transcription_prompt],
            config=config_transcribe
        )

        if not draft_response.text:
            raise ValueError("Model returned empty transcription.")

        # Step C: The Audit Pass
        audit_prompt = (
            "TASK: Review the following transcription of academic notes for syntax errors. "
            "Focus strictly on formatting:\n"
            "1. Ensure all LaTeX delimiters are balanced.\n"
            "2. Ensure tables use correct Markdown syntax.\n"
            "3. Fix any obvious spelling errors derived from handwriting misinterpretation, "
            "but only if the context makes the correction 100% certain.\n"
            "4. Output the final cleaned text only."
        )

        # --- BUFFER CHECK INSERTED HERE ---
        api_limiter.wait_for_slot()

        verified_response = client.models.generate_content(
            model=IngestionConfig.MODEL_FAST,
            contents=[f"DRAFT:\n{draft_response.text}", audit_prompt],
            config=types.GenerateContentConfig(temperature=IngestionConfig.TEMP_AUDIT)
        )

        final_text = verified_response.text if verified_response.text else draft_response.text

        # Step D: Atomic Write & Post-Process Validation
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Immediate syntax check before finalizing
        is_valid, reason = validate_syntax(final_text)
        
        target_file = output_path
        if not is_valid:
            # Divert to quarantine immediately
            IngestionConfig.QUARANTINE_DIR.mkdir(parents=True, exist_ok=True)
            target_file = IngestionConfig.QUARANTINE_DIR / output_path.name
            logger.warning(f"Quarantining {pdf_path.name}: {reason}")
        
        with open(target_file, "w", encoding="utf-8") as f:
            f.write(final_text)

        return True

    except Exception as e:
        # Log specific errors but allow the main loop to continue
        log_failure(pdf_path.name, "Processing", str(e))
        raise e # Re-raise to trigger the backoff logic in main
    finally:
        # Cleanup: Remove the file from cloud storage to maintain hygiene
        if myfile:
            try:
                client.files.delete(name=myfile.name)
            except Exception:
                pass

# --- 5. EXECUTION CONTROLLER ---

if __name__ == "__main__":
    # Ensure directories exist
    IngestionConfig.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # Filter: Process only PDFs that do not have a corresponding .txt file
    existing_stems = {
        f.stem for f in IngestionConfig.OUTPUT_DIR.glob("*.txt")
    } | {
        f.stem for f in IngestionConfig.QUARANTINE_DIR.glob("*.txt")
    }
    
    pdf_queue = [
        p for p in IngestionConfig.BASE_DIR.glob("*.pdf") 
        if p.stem not in existing_stems
    ]
    
    logger.info(f"Initialization Complete. Pending Documents: {len(pdf_queue)}")
    logger.info(f"Rate Limiter Enabled: {IngestionConfig.RPM_LIMIT} requests per {IngestionConfig.RPM_PERIOD} seconds.")

    for pdf in pdf_queue:
        output_txt = IngestionConfig.OUTPUT_DIR / f"{pdf.stem}.txt"
        
        # Exponential Backoff Strategy
        max_attempts = 3
        for attempt in range(max_attempts):
            try:
                logger.info(f"Processing: {pdf.name} (Attempt {attempt + 1})")
                process_document(pdf, output_txt)
                logger.info(f"Success: {pdf.name}")
                
                # Small operational pause (distinct from the API buffer)
                time.sleep(1) 
                break
                
            except exceptions.ResourceExhausted:
                # If we hit the limit despite the buffer, we perform a hard sleep
                wait_time = (2 ** attempt) * 60
                logger.warning(f"Rate Limit Hit (External). Pausing for {wait_time} seconds...")
                time.sleep(wait_time)
                
            except Exception as e:
                logger.error(f"Critical Failure on {pdf.name}: {e}")
                break # Move to next file on non-retryable error

    logger.info("Batch Processing Complete.")
